\documentclass[11pt]{article}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{array}
\usepackage{enumitem}
\setlist{nosep}

% ------------------------------------------------------------------
% Quick-edit macros (fill with actual values after running pipeline)
% ------------------------------------------------------------------
\newcommand{\numdocs}{31{,}842}
\newcommand{\numtokens}{11{,}905{,}937}
\newcommand{\numtypes}{586{,}674}
\newcommand{\heapsk}{17.89}
\newcommand{\heapsbeta}{0.640}
\newcommand{\bpetypes}{6{,}946}
\newcommand{\bpemerges}{5{,}000} % default in scripts
\newcommand{\spellaccone}{0.489}
\newcommand{\spellaccfive}{0.726}

\title{Azerbaijani Wikipedia Corpus: Tokenization, Heaps' Law, BPE, Sentence Segmentation, and Spell Checking}

\author{
    Team: \\
    \\
    \textbf{Kamal Ahmadov} \\
    kahmadov24700@ada.edu.az; kamal.ahmadov@gwu.edu \\
    \and
    \textbf{Rufat Guliyev} \\
    rguliyev24988@ada.edu.az; rufat.guliyev@gwu.edu
}
\date{February 5, 2026}

\begin{document}
\maketitle

\begin{abstract}
This report presents the creation of a large-scale Azerbaijani Wikipedia corpus and an NLP pipeline that includes tokenization, frequency analysis (Zipf's Law), Heaps' law estimation, Byte Pair Encoding (BPE), rule-based sentence segmentation, and a spell-checking system based on Levenshtein distance and weighted edit distance. All processing steps were automated, and the results were saved in structured directories. The report summarizes the methods, experimental setup, results, and key metrics generated during this study.
\end{abstract}

\section{Motivation and Dataset}
\textbf{Goal:} The primary objective is to build a comprehensive Azerbaijani corpus from Wikipedia, extract lexical statistics, and create preprocessing modules for common NLP tasks such as tokenization, segmentation, and spelling correction.

\textbf{Source:} The corpus was collected using the MediaWiki API, with cleaning performed through custom scripts. The data was saved as a CSV file at \texttt{data/raw/corpus.csv} containing document IDs, titles, revision info, timestamps, URLs, and text.

\textbf{Corpus Snapshot:}
\begin{itemize}
    \item \textbf{Documents:} \numdocs
    \item \textbf{Tokens:} \numtokens
    \item \textbf{Types (unique tokens):} \numtypes
\end{itemize}

\textbf{Licensing:} The corpus and any derived datasets adhere to the \href{https://creativecommons.org/licenses/by-sa/3.0/}{CC BY-SA} license. All derived artifacts must retain proper attribution.

\section{Methods}
\subsection{Tokenization}
We employed a Unicode-aware tokenizer that retains Azerbaijani characters, including internal apostrophes and hyphens, as well as decimal numbers. Non-alphabetic characters are removed, and case normalization (lowercasing) is optional. Additionally, Wikipedia-specific preprocessing removes category/navigation lines and normalizes punctuation (\texttt{src/tokenize.py}).

\subsection{Frequency Analysis and Zipf's Law}
We performed token and type frequency analysis, generating a full frequency table and identifying the top 20 tokens, stored in \texttt{outputs/stats/token\_freq.csv} and summarized in \texttt{summary.json}.
\begin{itemize}
    \item Zipf’s Law was visualized with a rank-vs-frequency plot, available in \texttt{outputs/plots/zipf.png}. The plot confirms a typical heavy-tail distribution, which is expected in natural language datasets.
\end{itemize}

\subsection{Heaps' Law}
Heaps' Law is used to model vocabulary growth as the corpus size increases. We computed the relationship between the number of tokens \( N \) and the number of unique words \( V \), fitting the model \( V = k N^{\beta} \) via linear regression in log space (\texttt{src/heaps.py}).
\begin{itemize}
    \item Heaps' parameters: \( k = \heapsk \), \( \beta = \heapsbeta \), indicating moderately fast vocabulary growth.
    \item The plot of Heaps' law is shown in \texttt{outputs/plots/heaps.png}.
    \item The chosen \( \beta \) value reflects the broad lexical breadth of the corpus, typical of encyclopedic text.
\end{itemize}

\subsection{Byte-Pair Encoding (BPE)}
We applied BPE to segment words into subword units, which improves handling of rare and out-of-vocabulary words.
\begin{itemize}
    \item \textbf{Parameters:} 5000 merges, minimum frequency threshold of 2 (\texttt{src/task3\_bpe.py}).
    \item Outputs include: \texttt{outputs/bpe/merges.txt}, \texttt{outputs/bpe/bpe\_token\_freq.csv}, and a BPE summary in JSON format.
    \item Results: \bpetypes\ subword types and 430,034 BPE tokens. This reduces the vocabulary size and allows the model to better handle rare words.
\end{itemize}

\subsection{Sentence Segmentation}
We implemented an enhanced rule-based approach to sentence segmentation. In this approach, sentence boundaries are detected using punctuation marks and contextual constraints. Specifically, we use the following boundary candidates:
\[
p \in \{.,!,?\}
\]

\textbf{We do NOT split when:}
\[
\text{Non-space on both sides of } p \Rightarrow \text{no split}
\]
\[
\text{Examples: } \texttt{3.14},\ \texttt{50.5},\ \texttt{S.Rustamov}
\]

\[
\text{Uppercase initial + period} \Rightarrow \text{no split}
\]
\[
\text{Example: } \texttt{A. Məlikli}
\]

\textbf{We split when:}
\[
\text{Closing quote + space + Uppercase letter} \Rightarrow \text{boundary}
\]

The enhanced segmentation rules address edge cases, including:
\begin{itemize}
    \item Abbreviations (e.g., \texttt{prof.}, \texttt{Dr.})
    \item Decimal numbers (e.g., \texttt{3.14}, \texttt{50.5})
    \item Initials (e.g., \texttt{S.Rustamov}, \texttt{A.M.})
    \item Quotes and guillemets: Closing quote followed by space and uppercase triggers sentence boundary
    \item Lowercase continuation after periods (e.g., \texttt{kv. verst}) is preserved to avoid false splits
\end{itemize}

These heuristics significantly reduce false sentence boundaries, especially in documents with numbers, personal names, and quotes.


\subsubsection*{Segmentation Evaluation Pipeline}
\begin{itemize}
    \item Tooling: \texttt{src/evaluate\_segmentation.py} (Precision/Recall/F1 via \texttt{sklearn} + BDER), helper wrapper \texttt{scripts/eval\_sentseg.sh}.
    \item Inputs: gold and predicted boundary indices (JSON array or newline-separated). BDER is defined as $(FP+FN)/|gold|$.
    \item Command example: \texttt{python -m src.evaluate\_segmentation --gold <gold\_idx> --pred <pred\_idx> --out outputs/sentseg\_eval.json}.
    \item If gold sentences exist (one per line), run \texttt{bash scripts/eval\_sentseg.sh data/processed/sent\_gold.txt 50} to extract subset, segment, convert to indices, and score.
    \item Status: pipeline ready; manually curated gold boundaries still needed (auto-generated \texttt{sent\_gold.txt} is a placeholder).
    \item Reported metrics (once gold is available) are stored in \texttt{outputs/sentseg\_eval.json}; fields include Precision, Recall, F1, and BDER.
\end{itemize}

\subsection{ Checking}
We developed an advanced spell-checking system that uses a combination of Levenshtein distance and weighted edit distance to improve accuracy. 
\\\\
The code scans the vocabulary and computes Levenshtein (or weighted) distance to the input word, keeping tokens within a max distance. It sorts candidates by distance, then frequency, and returns the top‑k suggestions with their counts. Also, length-based pruning is applied to filter out irrelevant candidates based on character length differences.
\\\\
Moreover, the spelling checker employs a sophisticated candidate generation mechanism to improve accuracy in handling common mistyped letters in Azerbaijani. Before querying the model with an input word, we identify problematic letters in the word that are most likely to be substituted with their Azerbaijani counterparts. 
\\\\
\textbf{Candidate Generation:}
\begin{itemize}
    \item We identify problematic letters in the input word (e.g., \{a, o, u, c, s, ch, sh, e, g\}), which are likely to be substituted with their Azerbaijani counterparts.
    \item A set of candidates is generated by substituting these characters in the input word in different combinations.
    \item For each candidate, 5 suggestions are retrieved from the model.
    \item These lists of suggestions are concatenated to form a combined candidate pool.
\end{itemize}


\indent\indent \textbf{Candidate Ranking:}
\begin{itemize}
    \item The lists of suggestions for each candidate are concatenated and sorted by the Levenshtein distance to \textit{any} of the generated candidates and their frequency.
    \item A list of the top 5 best suggestions is created out of the whole concatenated list and outputted as a result.
\end{itemize}

This approach significantly boosts the precision of the spell checker, improving success rates for words with common typing mistakes. By leveraging both character-level and frequency-based filtering, the model is able to produce more accurate spelling corrections, even for subtle or frequent letter swaps in the Azerbaijani language.



\section{Experiments and Results}
\subsection{Collection and Cleaning}
The corpus was gathered using a random/category fetch from MediaWiki, with templates and non-relevant tags stripped using \texttt{mwparserfromhell}. Category/file links were removed, and English-heavy lines were filtered using \texttt{langid}. Documents shorter than 400 characters were discarded.
\begin{itemize}
    \item Final corpus size: \numdocs\ documents, \numtokens\ tokens, \numtypes\ types.
    \item English-heavy lines were filtered to ensure the corpus contained mostly Azerbaijani text.
\end{itemize}

\subsection{Corpus Statistics}
The token and type statistics were recorded in \texttt{outputs/run\_summary.txt}:
\begin{itemize}
    \item \textbf{Tokens:} \numtokens
    \item \textbf{Types:} \numtypes
    \item Top tokens: Refer to \texttt{outputs/stats/summary.json} for the top-20 list.
\end{itemize}

\subsection{Heaps' Law}
The vocabulary growth parameter estimates are \( k = \heapsk \), \( \beta = \heapsbeta \), reflecting a vocabulary that grows faster than the classic value of 0.5. This is consistent with the corpus's encyclopedic nature. The Heaps' Law plot is shown in Figure~\ref{fig:heaps}.

\subsection{Zipf's Law}
The Zipf plot in Figure~\ref{fig:zipf} shows a typical rank-vs-frequency distribution, with a linear region confirming Zipf's Law behavior. The dataset’s vocabulary distribution follows the expected heavy-tail structure found in many natural language corpora.

\subsection{Byte-Pair Encoding (BPE)}
The BPE model performed 5,000 merges, generating 5,239 subword types. The total number of BPE tokens emitted was 430,034. This reduces the vocabulary size and facilitates more efficient handling of unseen words. Example segmentations are available in \texttt{outputs/bpe/bpe\_summary.json}.

\subsection{Sentence Segmentation}

Sentence segmentation was performed using a rule-based approach tailored to the characteristics of Azerbaijani Wikipedia text. From the first 500 documents, the segmenter extracted 11{,}479 sentences, producing well-formed sentence boundaries in the majority of cases.

A key challenge in this setting is that punctuation alone is an unreliable indicator of sentence boundaries. Azerbaijani Wikipedia articles frequently contain decimal numbers, abbreviations, personal initials, and quoted material, all of which can lead to erroneous splits under naive punctuation-based rules.

To address these issues, we enhanced the baseline segmenter with several language-aware heuristics. First, periods and commas are not treated as sentence boundaries when they are surrounded by non-space characters on both sides. This prevents incorrect segmentation in numerical expressions (e.g., \textit{3.14}) and compact abbreviations. Second, periods following a single uppercase letter are not interpreted as sentence boundaries, which improves handling of initial-based names such as \textit{K. Ahmadov}. Third, we introduce quotation-aware rules for guillemets and quotation marks. When a quoted segment ends with sentence-final punctuation, the segmenter inspects the following context: if the closing quote is followed by a space and a lowercase letter, no sentence boundary is introduced; if followed by a space and an uppercase letter, a sentence boundary is detected.

These enhancements substantially reduce false positives while preserving true sentence boundaries, particularly in texts containing numerical expressions, personal names, and embedded citations. Qualitative inspection shows that the enhanced segmenter handles lowercase continuations after periods (e.g., \textit{kv. verst}) correctly, with minimal remaining errors.



\subsection{Spell Checking}
The Levenshtein-based spell checker, enhanced with weighted edit distance, was evaluated on a synthetic test set of 1,000 misspelled words and achieved an accuracy of Accuracy@1 = \spellaccone\ and Accuracy@5 = \spellaccfive. A confusion matrix of top substitutions is visualized in Figure~\ref{fig:confusion}, highlighting common character substitutions.


\section{Reproducibility}
The full pipeline can be run with the following command:
\begin{verbatim}
bash scripts/run_all.sh
\end{verbatim}
Key outputs:
\begin{itemize}
    \item Plots: \texttt{outputs/plots/zipf.png}, \texttt{outputs/plots/heaps.png}
    \item Stats: \texttt{outputs/stats/summary.json}, \texttt{outputs/stats/heaps\_params.json}
    \item BPE: \texttt{outputs/bpe/merges.txt}, \texttt{outputs/bpe/bpe\_summary.json}
    \item Vocab: \texttt{data/processed/vocab.txt}
    \item Spellcheck eval: \texttt{outputs/spellcheck/spell\_eval.json}, \texttt{sample\_predictions.csv}, \texttt{confusion\_heatmap.png}
    \item Run summary: \texttt{outputs/run\_summary.txt}
\end{itemize}

\section{Discussion and Future Work}
\begin{itemize}
    \item Enhance data cleaning (address punctuation and diacritics).
    \item Implement more robust language-ID filtering to exclude non-Azerbaijani content.
    \item Train a neural segmenter or language model for improved performance.
    \item Expand spell checker with context-aware methods.
    \item Replace synthetic evaluation with human-annotated datasets for better performance.
\end{itemize}

\section{Figures}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{zipf.png}
    \caption{Zipf plot (rank vs.\ frequency, log--log).}
    \label{fig:zipf}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{heaps.png}
    \caption{Heaps' law fit with observed $V(N)$ and model $kN^{\beta}$.}
    \label{fig:heaps}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{confusion_heatmap.png}
    \caption{Top substitution confusions (weighted spell checker).}
    \label{fig:confusion}
\end{figure}

\end{document}
