\documentclass[11pt]{article}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{array}
\usepackage{enumitem}
\setlist{nosep}

% ------------------------------------------------------------------
% Quick-edit macros (fill with actual values after running pipeline)
% ------------------------------------------------------------------
\newcommand{\numdocs}{623}
\newcommand{\numtokens}{238{,}286}
\newcommand{\numtypes}{48{,}151}
\newcommand{\heapsk}{4.57}
\newcommand{\heapsbeta}{0.750}
\newcommand{\bpetypes}{5{,}239}
\newcommand{\bpemerges}{5{,}000} % default in scripts
\newcommand{\spellaccone}{0.637}
\newcommand{\spellaccfive}{0.801}

\title{Azerbaijani Wikipedia Corpus: Tokenization, Heaps' Law, BPE, Sentence Segmentation, and Spell Checking}
\author{Team: \textbf{[Names Here]}}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
We construct a lightweight Azerbaijani Wikipedia corpus and build a reproducible NLP pipeline covering tokenization, frequency analysis (Zipf), Heaps' law estimation, byte-pair encoding (BPE), rule-based sentence segmentation, and Levenshtein-based spell checking with weighted edit distance. All stages are scripted; outputs are saved under \texttt{outputs/} and \texttt{data/processed/}. This report summarizes motivation, methods, experiments, and key metrics needed for the course submission.
\end{abstract}

\section{Motivation and Dataset}
\textbf{Goal.} Create a course-scale Azerbaijani corpus, study its lexical statistics, and prototype fundamental preprocessing modules required for downstream NLP tasks (tokenization, segmentation, spelling).

\textbf{Source.} Azerbaijani Wikipedia pages collected via MediaWiki API (\texttt{src/pull\_wikipedia.py}) with cleaning (\texttt{src/clean\_corpus.py}). Output CSV: \texttt{data/raw/corpus.csv} (\texttt{doc\_id}, title, revision, timestamp, URL, text).

\textbf{Corpus snapshot (fill after running \texttt{scripts/run\_all.sh}):}
\begin{itemize}
    \item Documents: \numdocs
    \item Tokens: \numtokens
    \item Types (unique tokens): \numtypes
\end{itemize}

\textbf{Licensing.} CC BY-SA (Wikipedia). Derived artifacts must preserve attribution and ShareAlike.

\section{Methods}
\subsection{Tokenization}
\begin{itemize}
    \item Unicode-aware regex keeps Azerbaijani letters, internal apostrophes/hyphens, and decimal numbers; lowercasing optional.
    \item Wikipedia-specific cleanup removes category/navigation lines and normalizes punctuation (\texttt{src/tokenize.py}).
\end{itemize}

\subsection{Frequency Analysis and Zipf}
\begin{itemize}
    \item Token/type counts and full frequency table: \texttt{outputs/stats/token\_freq.csv}. Includes top-20 preview in \texttt{summary.json}.
    \item Zipf plot: \texttt{outputs/plots/zipf.png} (log--log rank vs.\ frequency). Straight-ish mid-section confirms expected heavy-tail.
\end{itemize}

\subsection{Heaps' Law}
\begin{itemize}
    \item Stream tokens; record $(N,V)$ every 1000 tokens; fit $V = k N^{\beta}$ via linear regression in log space (\texttt{src/heaps.py}).
    \item Parameters stored in \texttt{outputs/stats/heaps\_params.json}. Our run: $k=\heapsk$, $\beta=\heapsbeta$, indicating moderately fast vocabulary growth (above the classic 0.5).
    \item Plot: \texttt{outputs/plots/heaps.png}.
\end{itemize}

\subsection{Byte-Pair Encoding (BPE)}
\begin{itemize}
    \item Train merges on word tokens (default 5000 merges, min freq 2) (\texttt{src/task3\_bpe.py}).
    \item Outputs: \texttt{outputs/bpe/merges.txt}, \texttt{outputs/bpe/bpe\_token\_freq.csv}, summary JSON with example segmentations.
    \item Learned subword types: \bpetypes\ (see \texttt{bpe\_summary.json}); total BPE tokens emitted: 430{,}034.
\end{itemize}

\subsection{Sentence Segmentation}
\begin{itemize}
    \item Rule-based segmenter (\texttt{src/sentence\_segment.py}) handling abbreviations, decimals, initials, quote boundaries, and the ``lowercase-following-period'' rule (e.g., ``kv. verst'' stays unsplit).
    \item CLI: \texttt{python -m src.sentence\_segment --corpus\_path data/raw/corpus.csv --limit 500 --out outputs/sentences.txt}.
    \item Edge cases covered by regression tests: abbreviations (\texttt{prof.}), decimals, initials (\texttt{S.Rustamov}), and lowercase continuations after periods.
\end{itemize}

\subsection{Spell Checking}
\begin{itemize}
    \item Vocab from corpus, filtered by frequency/length; default edit distance cutoff 2.
    \item Baseline distance: uniform Levenshtein (\texttt{src/levenshtein.py}); weighted distance via character-level confusion costs (\texttt{src/weighted\_levenshtein.py}) learned from synthetic errors.
    \item Synthetic misspelling set: \texttt{data/processed/spell\_test.csv} (1000 corruptions sampled by frequency).
    \item Confusion matrix/weights: \texttt{outputs/spellcheck/confusion.json}; heatmap: \texttt{outputs/spellcheck/confusion\_heatmap.png}.
    \item Evaluation: \texttt{outputs/spellcheck/spell\_eval.json}; current weighted results: Accuracy@1=\spellaccone, Accuracy@5=\spellaccfive\ on $n=1000$.
\end{itemize}

\section{Experiments and Results}
\subsection{Collection \& Cleaning}
\begin{itemize}
    \item MediaWiki random/category fetch; templates/tags stripped with \texttt{mwparserfromhell}; category/file links removed; English-heavy lines filtered with \texttt{langid}; short docs $<400$ chars dropped.
    \item Final corpus: \numdocs\ documents; \numtokens\ tokens; \numtypes\ types.
\end{itemize}

\subsection{Corpus Statistics}
Key figures (from \texttt{outputs/run\_summary.txt}):
\begin{itemize}
    \item Documents: \numdocs
    \item Tokens: \numtokens; Types: \numtypes
    \item Top tokens: see \texttt{outputs/stats/summary.json} (top\_20 list).
\end{itemize}

\subsection{Heaps' Law}
$k=\heapsk$, $\beta=\heapsbeta$ (above the classic 0.5, reflecting encyclopedia-style lexical breadth). Plot in Figure~\ref{fig:heaps}.

\subsection{Zipf Plot}
Figure~\ref{fig:zipf} shows rank--frequency on log--log axes; near-linear section indicates Zipf-like behavior.

\subsection{BPE}
Total merges: \bpemerges; subword vocabulary size: \bpetypes; total BPE tokens: 430{,}034. Examples in \texttt{outputs/bpe/bpe\_summary.json}.

\subsection{Sentence Segmentation}
Qualitative spot checks: abbreviations (``prof.''), decimals, initials, ``kv. verst'' continuation; 11{,}479 sentences extracted from first 500 docs (\texttt{outputs/sentences.txt}). Gold evaluation pending manual curation (\texttt{data/processed/sent\_gold.txt}).

\subsection{Spell Checking}
Weighted edit distance improves robustness on observed confusions. Current synthetic benchmark: Accuracy@1=\spellaccone, Accuracy@5=\spellaccfive\ (uniform character costs are worse; see \texttt{spell\_eval.json}). Heatmap of top substitutions in Figure~\ref{fig:confusion}.

\section{Reproducibility}
Run the full pipeline (assumes \texttt{data/raw/corpus.csv} exists):
\begin{verbatim}
bash scripts/run_all.sh
\end{verbatim}
Key outputs:
\begin{itemize}
    \item Plots: \texttt{outputs/plots/zipf.png}, \texttt{outputs/plots/heaps.png}
    \item Stats: \texttt{outputs/stats/summary.json}, \texttt{outputs/stats/heaps_params.json}
    \item BPE: \texttt{outputs/bpe/merges.txt}, \texttt{outputs/bpe/bpe_summary.json}
    \item Vocab: \texttt{data/processed/vocab.txt}
    \item Spellcheck eval: \texttt{outputs/spellcheck/spell_eval.json}, \texttt{sample_predictions.csv}, \texttt{confusion_heatmap.png}
    \item Run summary (ready-to-quote): \texttt{outputs/run\_summary.txt}
\end{itemize}

\section{Discussion and Future Work}
\begin{itemize}
    \item Improve cleaning/normalization (diacritics, punctuation variants).
    \item Add language-ID filtering per sentence to drop non-AZ content earlier.
    \item Train a small language model or neural segmenter as a stronger baseline.
    \item Expand spell checker with context-aware ranking (language model scoring) and real-world typo collection.
    \item Human-annotate sentence boundaries and real misspellings to replace synthetic evaluation.
\end{itemize}

\section{Figures}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{../outputs/plots/zipf.png}
    \caption{Zipf plot (rank vs.\ frequency, log--log).}
    \label{fig:zipf}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{../outputs/plots/heaps.png}
    \caption{Heaps' law fit with observed $V(N)$ and model $kN^{\beta}$.}
    \label{fig:heaps}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{../outputs/spellcheck/confusion_heatmap.png}
    \caption{Top substitution confusions (weighted spell checker).}
    \label{fig:confusion}
\end{figure}

\end{document}
