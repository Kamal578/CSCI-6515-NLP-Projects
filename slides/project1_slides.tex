\documentclass{beamer}
\usetheme{Madrid}
\usecolortheme{default}
\setbeamertemplate{footline}[frame number]

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath, amssymb}
\usepackage{mathtools}
\usepackage{listings}

\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single
}


\title{Spring 2026 Natural Language Processing (CSCI-6515)}
\subtitle{Azerbaijani Wikipedia Corpus: Tokenization, Heaps' Law, BPE, Sentence Segmentation, and Spell Checking}
\author{
\textbf{Kamal Ahmadov} \\ \small kahmadov24700@ada.edu.az; kamal.ahmadov@gwu.edu \\ \and
\textbf{Rufat Guliyev} \\ \small rguliyev24988@ada.edu.az; rufat.guliyev@gwu.edu
}
\institute{ADA University \& George Washington University}
\date{February 5, 2026}

\begin{document}

%------------------------------------------------
\begin{frame}
  \titlepage
\end{frame}


%------------------------------------------------
\begin{frame}{Motivation}
\begin{itemize}
  \item Azerbaijani is a low-resource language for NLP research
  \item Wikipedia provides freely licensed, diverse text data (CC BY-SA)
  \item Goal: build a clean corpus and implement core preprocessing modules
  \item Outputs are fully reproducible via \texttt{bash scripts/run\_all.sh}
\end{itemize}
\end{frame}

%------------------------------------------------
\begin{frame}{Dataset Snapshot}
\begin{itemize}
  \item Source: Azerbaijani Wikipedia (MediaWiki API)
  \item Cleaned corpus stored as CSV: \texttt{data/raw/corpus.csv}
  \item Final size:
  \begin{itemize}
    \item 31{,}842 documents
    \item 11{,}905{,}937 tokens
    \item 586{,}674 unique token types
  \end{itemize}
\end{itemize}
\end{frame}

%------------------------------------------------
\begin{frame}{Cleaning Challenges}
\begin{itemize}
  \item Wikipedia markup noise:
  \begin{itemize}
    \item templates, categories, navigation text, references
  \end{itemize}
  \item Mixed-language artifacts (English-heavy references)
  \item Broken punctuation / formatting residue
  \item Short or empty pages removed to improve quality
\end{itemize}
\end{frame}

%------------------------------------------------
\begin{frame}{Tokenization (Unicode-aware)}
We tokenize using Unicode letter categories and preserve Azerbaijani characters.

\[
\text{token} \in \{\text{letters}\}^{+}
\quad \text{with optional internal } ['-]
\]

Example:
\[
\texttt{``S.Rustamov 2.5 dəfə artırsaq''}
\Rightarrow
[\texttt{S.Rustamov},\ 2.5,\ \texttt{dəfə},\ \texttt{artırsaq}]
\]

\begin{itemize}
  \item Handles diacritics: \texttt{ə, ğ, ö, ş, ü, ı, ç}
  \item Keeps decimals to avoid splitting numeric expressions
  \item Lowercasing used for vocabulary normalization
\end{itemize}
\end{frame}

%------------------------------------------------
\begin{frame}{Zipf's Law (Rank--Frequency)}
\begin{columns}
\column{0.52\textwidth}
\begin{itemize}
  \item Empirical law for word frequencies
  \item Frequency decays with rank
\end{itemize}

\[
f(r) \propto \frac{1}{r^{s}}
\quad\Rightarrow\quad
\log f(r) = c - s \log r
\]

\begin{itemize}
  \item Near-linear mid section on log--log plot
  \item Confirms heavy-tailed behavior in our corpus
\end{itemize}

\column{0.48\textwidth}
\centering
\includegraphics[width=\linewidth]{zipf.png}
\end{columns}
\end{frame}

%------------------------------------------------
\begin{frame}{Heaps' Law (Vocabulary Growth)}
\begin{columns}
\column{0.55\textwidth}
\[
V(N) = kN^{\beta}
\quad\Rightarrow\quad
\log V = \log k + \beta \log N
\]

\begin{itemize}
  \item Sample $(N,V)$ every 1000 tokens
  \item Fit $\log V$ vs.\ $\log N$ by linear regression
\end{itemize}

\[
k = 17.89,\qquad \beta = 0.640
\]

\begin{itemize}
  \item $\beta>0.5$ indicates fast vocabulary growth
  \item Encyclopedic topics + named entities increase diversity
\end{itemize}

\column{0.45\textwidth}
\centering
\includegraphics[width=\linewidth]{heaps.png}
\end{columns}
\end{frame}

%------------------------------------------------
\begin{frame}{Byte Pair Encoding (BPE)}
BPE learns subword units by repeatedly merging the most frequent symbol pairs.

\[
(a,b) = \arg\max_{(x,y)} \text{freq}(x\,y)
\quad\Rightarrow\quad
xy \rightarrow \langle xy \rangle
\]

\begin{itemize}
  \item Merges: 5000 \quad $\rightarrow$ \quad Subword vocab: 6946
  \item Reduces OOV rate and helps with morphology
\end{itemize}

Example:
\[
\texttt{azərbaycanlılardan}
\Rightarrow
\texttt{azərbaycan} + \texttt{lı} + \texttt{lardan}
\]
\end{frame}

%------------------------------------------------
\begin{frame}{Enhanced Rule-Based Sentence Segmentation}

Sentence boundaries are detected using punctuation
\[
p \in \{.,!,?\}
\]
\textbf{only when contextual constraints are satisfied.}

\vspace{0.3cm}

\textbf{We block sentence boundaries if:}
\[
\text{Non-space on both sides of } p
\Rightarrow \text{no split}
\]
\[
\text{Examples: } \texttt{3.14},\ \texttt{50.5},\ \texttt{S.Rustamov}
\]

\[
\text{Uppercase initial + period}
\Rightarrow \text{no split}
\]
\[
\text{Example: } \texttt{A. Məlikli}
\]

\end{frame}

%------------------------------------------------
\begin{frame}{Segmentation Challenges and Heuristics}
\small
\begin{itemize}
  \item Abbreviations: block splits after \texttt{dr., prof., mr., etc.}
  \item Decimals / numbers: no split inside \texttt{3.14}, \texttt{50,5}
  \item Initials / acronyms: no split inside \texttt{S.Rustamov}, \texttt{A.M.}
  \item Quotes: closing quote + space + lowercase $\Rightarrow$ no split; + uppercase $\Rightarrow$ split
  \item Lowercase continuation after period: keep \texttt{kv. verst} unsplit
\end{itemize}
\vspace{0.2cm}
Lightweight rules reduce false boundaries in encyclopedic text while remaining fast and interpretable.
\end{frame}

%------------------------------------------------
\begin{frame}{Quotation-Aware and Context-Sensitive Splitting}

Special handling is applied to quoted text and citations.

\vspace{0.3cm}

\textbf{Quotation rule:}
\[
\text{closing quote} + \text{space} + \text{Uppercase}
\Rightarrow \text{sentence boundary}
\]

\[
\text{closing quote} + \text{space} + \text{lowercase}
\Rightarrow \text{no split}
\]

\vspace{0.4cm}

\textbf{This prevents over-splitting in:}
\begin{itemize}
  \item Embedded citations
  \item Lowercase continuations (e.g., \texttt{kv. verst})
  \item Azerbaijani quotation styles (", « », etc.)
\end{itemize}

\end{frame}

%------------------------------------------------
\begin{frame}{Segmentation Evaluation}
\small
\begin{itemize}
  \item Metrics: Precision, Recall, F1 (sklearn) and BDER $= (FP+FN)/|gold|$
  \item Inputs: gold + predicted boundary indices (JSON or newline list)
  \item Tooling: \texttt{python -m src.evaluate\_segmentation ...}; wrapper \texttt{bash scripts/eval\_sentseg.sh <gold\_sentences> <limit>}
  \item Status: pipeline ready; awaiting manually curated gold set (auto gold used only for scaffolding)
\end{itemize}
\end{frame}

%------------------------------------------------
\begin{frame}{Why These Rules Matter}

\begin{itemize}
  \item Azerbaijani Wikipedia contains dense punctuation and abbreviations
  \item Naive punctuation splitting produces many false sentence boundaries
  \item Our rules significantly reduce:
  \begin{itemize}
    \item False splits in names and numbers
    \item Errors in quoted material
  \end{itemize}
  \item Lightweight, interpretable, and fast
  \item Strong baseline for low-resource languages
\end{itemize}

\end{frame}


%------------------------------------------------
\begin{frame}{Spell Checking: Levenshtein Distance}
Baseline: Levenshtein distance via dynamic programming.

\[
D(i,j)=\min
\begin{cases}
D(i-1,j)+1 & \text{delete} \\
D(i,j-1)+1 & \text{insert} \\
D(i-1,j-1)+[x_i \neq y_j] & \text{substitute}
\end{cases}
\]

\begin{itemize}
  \item Given misspelled token $w$, search candidates $c \in V$
  \item Rank by smallest $D(w,c)$ (with pruning / max distance)
\end{itemize}
\end{frame}

%------------------------------------------------
\begin{frame}{Spell Checking: Candidate Generation}

Before querying the model, we generate candidates by focusing on common letter substitutions in Azerbaijani:
\begin{itemize}
  \item Problematic letters: \{ a, o, u, c, s, ş, ç, e, ğ \}
  \item Substituting these letters gives multiple possible candidates
  \item For each candidate, we retrieve 5 suggestions from the model
\end{itemize}
\end{frame}

%------------------------------------------------
\begin{frame}{Spell Checking: Ranking and Pruning Candidates}

After generating multiple candidates:
\begin{itemize}
  \item Concatenate all candidate lists
  \item Rank candidates by Levenshtein distance and frequency in the dataset
  \item Prune irrelevant candidates based on character length
\end{itemize}

\textbf{Final suggestion:}
\begin{itemize}
  \item Top-5 words with shortest distance and highest frequency
\end{itemize}
\end{frame}

%------------------------------------------------
\begin{frame}{Spell Checking: Impact on Accuracy}

\begin{itemize}
  \item Precision and recall greatly improved due to:
  \begin{itemize}
    \item Candidate generation via substitution
    \item Higher quality model suggestions
  \end{itemize}
  \item Top-5 suggestions highly reliable, reducing false positives
\end{itemize}
\end{frame}

%------------------------------------------------
\begin{frame}{Spell Checker: Final Recap}

\textbf{Candidate Generation:}
\begin{itemize}
  \item Letter substitution for common spelling errors
  \item Generate candidates by substituting frequent miswritten characters
\end{itemize}

\textbf{Ranking & Final Suggestions:}
\begin{itemize}
  \item Top-5 ranked suggestions from model, based on Levenshtein distance and frequency
\end{itemize}

\end{frame}


%------------------------------------------------
\begin{frame}{Weighted Edit Distance (From Confusions)}
\small

We replace uniform substitution costs with learned costs from character confusions.

\[
\text{cost}(a \rightarrow b) = \frac{1}{\epsilon + P(b|a)}
\]

\medskip
\textbf{Weighted recurrence:}

\[
D(i,j)=\min
\begin{cases}
D(i-1,j)+w_{\text{del}}(x_i) \\
D(i,j-1)+w_{\text{ins}}(y_j) \\
D(i-1,j-1)+w_{\text{sub}}(x_i,y_j)
\end{cases}
\]

\end{frame}

%------------------------------------------------
\begin{frame}{Weighted Edit Distance (Interpretation)}
\small
\begin{columns}
\column{0.52\textwidth}
\begin{itemize}
  \item Confusions learned from synthetic misspellings
  \item Common substitutions get lower penalty
  \item Diacritic and keyboard-adjacent errors favored
  \item Improves ranking of realistic corrections
\item Candidate pruning: length filter ($|\,|w|-|c|\,|\leq$ max\_dist) before scoring; ranking by (distance, freq)
\end{itemize}

\column{0.48\textwidth}
\centering
\includegraphics[width=\linewidth]{confusion_heatmap.png}
\end{columns}
\end{frame}



%------------------------------------------------
\begin{frame}{Evaluation Metrics}
We evaluate spellchecker ranking using Accuracy@k.

\[
\text{Acc@k} = \frac{1}{n}\sum_{i=1}^{n}
\mathbf{1}\{y_i \in \text{TopK}(x_i)\}
\]

Results on 1000 synthetic misspellings:
\[
\text{Acc@1}=0.489,\qquad \text{Acc@5}=0.726
\]

\begin{itemize}
  \item Acc@1 = strict correction success
  \item Acc@5 = practical usefulness of suggestions
\end{itemize}
\end{frame}

%------------------------------------------------
\begin{frame}{Spellcheck Pipeline Recap}
\small
\begin{itemize}
  \item Vocab filtering: Azerbaijani alphabet only, min\_freq (2), min\_len (3)
  \item Candidate pruning: length difference $\leq$ max\_dist (2) before distance
  \item Distance: Levenshtein; weighted edits if confusion weights provided
  \item Ranking: (distance asc, frequency desc), top-5 suggestions
  \item Rare-token scan: report suggestions for 200 rarest tokens to spot likely typos
\end{itemize}
\end{frame}

%------------------------------------------------
\begin{frame}{Key Outputs (Reproducibility)}
\begin{itemize}
  \item Run everything:
  \begin{itemize}
    \item \texttt{bash scripts/run\_all.sh}
  \end{itemize}
  \item Plots:
  \begin{itemize}
    \item \texttt{outputs/plots/zipf.png}, \texttt{outputs/plots/heaps.png}
  \end{itemize}
  \item BPE:
  \begin{itemize}
    \item \texttt{outputs/bpe/merges.txt}, \texttt{bpe\_summary.json}
  \end{itemize}
  \item Sentence segmentation:
  \begin{itemize}
    \item \texttt{outputs/sentences.txt}, optional eval \texttt{outputs/sentseg\_eval.json}
  \end{itemize}
  \item Spellcheck:
  \begin{itemize}
    \item \texttt{spell\_eval.json}, \texttt{confusion\_heatmap.png}
  \end{itemize}
\end{itemize}
\end{frame}

%------------------------------------------------
\begin{frame}{Conclusion}
\begin{itemize}
  \item Built a clean Azerbaijani Wikipedia corpus (31{,}842 docs)
  \item Verified Zipf-like distribution and fitted Heaps' law
  \item Trained BPE subword model for robust tokenization
  \item Implemented rule-based sentence segmentation with edge-case handling
  \item Developed spellchecker with weighted edit distance; Acc@5 = 0.726
\end{itemize}
\end{frame}

%------------------------------------------------
\begin{frame}{Future Work}
\begin{itemize}
  \item Improve cleaning and Azerbaijani-only filtering earlier
  \item Replace synthetic gold data with manual annotations
  \item Context-aware spell correction using language model scoring
  \item Explore neural sentence segmentation as stronger baseline
\end{itemize}
\end{frame}

%------------------------------------------------
\begin{frame}
\centering
\Huge Questions?
\end{frame}

\end{document}
